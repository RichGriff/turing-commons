# Sample Claims

This is a supplementary document to be used alongside the [Selecting and Evaluating Claims](rri-204-a1.md) activity.

Each claim in the following set is about a hypothetical project, model, or system.
Various details have been omitted to focus on the claim itself.
You should not need to know anything beyond what is expressed in the claim itself to complete the associated activity.
Instructions are available [here](rri-204-a1.md).

There are 15 claims in total.
5 claims have been designed as examples of 'project transparency' explanations, 5 claims have been designed as 'model interpretability' explanations, and 5 claims have been designed as examples of 'situated explanations'.
However, the explanations also differ in terms of their quality, so some may be harder to identify than others.

Answers can be found using the 'Reveal Answers' admonition at the bottom of this document, if accessing online.

## Set of Example Claims

!!! info

    The following claims are not in any particular order.

1. "We chose to create a secure environment for performing data processing (e.g. extraction, analysis, and preprocessing) because our data is highly sensitive. In addition, this level of security prevents us from making information about our pipeline available to the public."
2. "We chose to use a neural network model because it gives us the highest levels of accuracy."
3. "Our stakeholder analysis process identified several groups of stakeholders, including the general public. To ensure that we met the different explainability requirements, we explored multiple processes for developing accessible explanations. For example, technical auditors have access to the results of complemenatry interpretability methods (i.e. Integrated Gradients and SHAP), while members of the public can access general information about how our system was built and implemented."
4. "Because of the context in which our system is intended to be used, there was a risk that our system could have lower levels of accuracy for specific sub-groups. Therefore, we have ensured that the information about our training data and a variety of feature summary statistics are available to assist bias mitigation and assessment efforts."
5. "We used Gradient-weighted Class Activation Mapping (Grad-CAM)to help visualise the regions of an image that were most important for a particular prediction made by a neural network. We chose this technique because it is effective at generating visual explanations that are easy to understand and interpret, even for non-technical stakeholders."
6. "


??? success

    1. Project Transparency (Medium Quality)
    2. Model Interpretability (Lower Quality)
    3. Situated Explanation (Higher Quality)
    4. Situated Explanation (Medium Quality)
    5. Model Interpretability (Medium Quality)
    6. 