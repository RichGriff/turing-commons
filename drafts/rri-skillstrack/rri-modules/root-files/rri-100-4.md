---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# What is Responsible Research and Innovation? (Section 4) The Scope and Horizon of Responsibility

> **Note**
> The following sections are part of this module:
>
> - Section 1: [Understanding Responsibility](rri-100-1.md)
> - Section 2: [Collective and Distributed Responsibility](rri-100-2.md)
> - Section 3: [Defining Responsible Research and Innovation](rri-100-3.md)
> - Section 4: [The Scope and Horizon of Responsibility](rri-100-4.md)

---

## The boundaries of responsibility

Consider the following scenario.

A research team have developed a predictive model, which is well-validated in some narrow context (e.g. for diagnosing an illness in a well-defined cohort of patients; for predicting house prices in a specific locale). They publish their data (withholding any sensitive information), method, and model according to some widely agreed protocol or set of practices (e.g. for reproducible, replicable, and open science). They also carefully set out the limitations of their model, including the limits of their model's generalisability without further re-training and validation. Finally, they include a permissive license on reuse, but also set out specific restrictions for "non-permitted" uses of their model based on their own risk assessment activities.

----

Now, consider the following questions:

1. Are the team responsible for harms caused by a company who try to commercialise their model by embedding it into a system that is one of the teams "non-permitted" uses?
2. Are the team responsible for any harm caused by another research team who deploy the original model in a new context without carrying out the suggested re-training (with representative data) and validation of the model in the new context?
3. Are the team responsible for their model leaking sensitive information due to the manner in which they trained the model?[^leaks]
4. Are the team responsible for the unintended publication of sensitive data due to human error when publishing their project repository online?

----

These questions all pertain to the scope or limit of the team's responsibility for any harms caused by their activities.
Although individual answers to these questions may vary, they are likely to follow a trend:

1. Least likely to be responsible
2. [...]
3. [...]
4. Most likely to be responsible

In the first case, it is quite obvious that the company are using the model in ways that the team had taken steps to prevent (e.g. license of reuse).
And, in the second case, the project team had also set out careful limitations on their model by following well-established protocols.
But, the third case is where we enter into more of a gray area.

----

Whether you believe the project team to be responsible, whether fully or partially, for the leakage in the third case will depend on several factors.
For example, in this case the issue seems to arise from a design choice made during model training, which could have arisen due to a lack of knowledge within the team about the vulnerabilities of certain training methods.
Given the fast-changing nature of data science and AI, knowledge of vulnerabilities such as a model leaking information about its training data can remain unknown to teams who do not have the appropriate knowledge, skills, or resources.
You, the reader, may be more or less sympathetic to the challenge of keeping abreast of this fast-paced field, and this will undoubtedly influence your decision.

----

But, in the fourth case, most are likely to ascribe responsibility to some member of the team.
Maybe the project manager should have had greater oversight of the team's data management?
Maybe there should have been safeguards to prevent this from occurring (e.g. secure research environments that prevent egress of sensitive data)?
Maybe the individual who caused the publication should have just been paying closer attention!?

The specific answer does not matter here.
What is important is to acknowledge that the demands of responsibility have boundaries, and drawing (or, delineating) these boundaries requires careful reflection and inclusive deliberation.

In the following sections we will explore these boundaries further.

<!-- Admonition -->
**Ought Implies Can**
A widely agreed upon moral standard, attributable to the philosopher Immanuel Kant, which is relevant here is the precept, "Ought Implies Can".
In short, you are only morally responsible or obligated to perform some action if you are capable of doing so.
For example, you would not be responsible for saving a dying person if you lacked the required medical training.
And, to flip this example on its head, you would be acting *irresponsibly* if you tried, say, to carry out surgery on someone without having undergone the necessary surgical training, no matter how beneficial your intentions.

Rules and principles such as 'ought implies can' also serve to set boundaries on our moral duties, responsibilities, and obligations.

---

## To whom are we responsible?

Let's start with a thought experiment from the moral philosopher, Peter Singer, which addresses the spatial (or geographic) dimension of moral responsibility. Consider the following scenario:

On your way to walk each day you walk along a river.
One morning, you spot a child that has fallen in and appears to be drowning. Saving the child would result in you ruining your clothes and being late to work.

Singer asks, 'do you have any obligation to jump in and rescue the child?'

When asked this question, almost all of us would answer with a resounding "yes", except where there are overriding factors (e.g. being unable to swim yourself—another example of the 'ought implies can' precept).

----

Singer continues, 'does the cost of your clothes affect your decision, or would it make a difference that there are other people walking past the pond who would equally be able to rescue the child but are not doing so?'

Again, most of us would say, 'no, the cost of clothes should not be valued beyond the life of a child, and it does not matter that others are not acting as they ought to do'. Another strike against the diffusion of responsibility.

----

And then we come to the crux of the thought experiment: what if the child were far away in another country, and while not drowning, we could save their life at virtually no cost to ourselves?

If proximity or distance make no difference to our moral consideration, as Singer would argue, then as he explains,

> 'we are all in that situation of the person passing the shallow pond: we can all save lives of people, both children and adults, who would otherwise die, and we can do so at a very small cost to us'

Whether for the cost of a coffee, a mobile app, or another subscription service, global aid charities can help us save the lives of those in need.
But what is the extent of our obligations to those we are not in immediate contact with?

----

While not as extreme as the above thought experiment, many projects involving data-driven technologies give rise to similar challenges about the scope of our moral obligations.
For example, the following scenarios all emphasise salient factors related to our capacity for moral concern and the effect of spatial separation:

- The data analyst who is physically separated from the subjects in her dataset, viewing them merely as numerical representations on a screen, may be less likely to extend them the same moral consideration for privacy or informed consent as the scientist gathering data during an in-person experiment.
- The software engineer developing a biometric identity system for a national agency, may be unable to fully appreciate the impact that such a system will have on people from poorer backgrounds who have lower levels of digital literacy or access to technology.
- The climate scientist deciding where to place environmental monitoring systems to record pollution levels may be unable to access demographic information that would reveal how their choices, which are guided only by considerations of maximising coverage, nevertheless fail to monitor neighbourhoods that are overwhelmingly populated by minority groups.

Understanding the impact that data-driven technologies could have on society, especially those with international scope (e.g. social media platforms), requires us to consider individuals and communities beyond our immediate sphere of concern.
And, in doing so, we often find ourselves drawing an implicit boundary around those we consider and those we do not.

----

<!-- admonition -->
**Drawing Boundaries**

This section will not attempt to provide any general guidance on drawing boundaries in practice—beyond the simple precepts such as 'ought implies can'.
To do so without first considering the context-specific factors of a particular project would either a) take us too far afield into moral philosophy, or b) simply cause confusion and tangential discussion points.
It is sufficient for our purpose to just draw attention to the issue.
Our skills track on [AI Ethics and Governance](/docs/aeg/index.md), however, goes into more detail on relevant topics.

---

## Responsibility for Future Generations

We have explored the spatial dimension of our sphere of moral concern; what about the temporal dimension?
How far into the future should or does our responsibility extend?

----

One way to think about this question, popular among decision theorists, is to adopt a mathematical function that represents your level of responsibility as a function of time and increasing uncertainty.
For instance, you may think that because it is increasingly difficult to be certain of the consequences of your actions as time increases (recall the AREA framework) that responsibility should decline *proportional* to the increase of uncertainty.
But what is the shape or properties of such a function?

----

Two possible options include the function shown in graph A, which represents your level of responsibility as a function that decreases *linearly* over time, and graph B, which represents it as a function that decreases *non-linearly* over time.

![](https://i.imgur.com/lrcgihb.png)

----

These are just two options among many, and omit many questions and details:

- Once we have selected a function, how should we then operationalise the decrease in responsibility and, say, construct a measure to guide decision-making (e.g. an increasing reduction in the time spent deliberating about possible impacts for longer-term consequences)
- How would such a function interact with the spatial dimension? Should groups of people that are separated from us both spatially and temporally receive less consideration (e.g. unborn people across the globe)?
- Do organisations with access to greater resources have a commitment to consider a wider and deeper range of impacts (e.g. social media companies or multi-national technological companies that create globally impactful systems)? To what extent does this commitment exceed the commitment of a small, national research team?

---

### Next Steps

In this module, we have identified a series of challenges but have not considered how to address them.
This is intentional.
Our goal so far has been to *understand* the challenges at a conceptual level. We have not looked at how we can address the challenges in practical terms.

As such, you have encountered many questions without any answers.
Fear not, as we move into the later sections, you will begin to encounter practical tools and procedures that can help you address similar challenges when they arise in your own projects.

---

[^leaks]: [This article](https://pair.withgoogle.com/explorables/data-leak/) form the PAIR team at Google provides a simple illustration of this phenomenon.
