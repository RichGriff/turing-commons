---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 2): Project Transparency?

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situating Explanations](rri-203-4.md)

---

## Should we in fact be trying to explain the data?

> Sometimes, we are asked to explain why the predictions being generated from a model are different in distribution from those observed in the past. Perhaps our model is recommending scarves and winter gloves instead of t-shirts and swimsuits. By inspecting our global model explanation we might observe that ‘season’ is an important feature and conclude that transitioning from Summer to Autumn has a significant impact on people’s behaviour. Here, it might be obvious that the input data that has changed and explains the change in model output.
> 
> Now, say the conversion rate has dropped for our product recommendation model. On the surface this may feel like another ‘explain the model’ problem, but consider the case where a stage in our ML pipeline breaks, perhaps the data stops being recorded properly and the ‘age’ feature gets set to a default value of NA or 0. Very likely, our model will start recommending products to adults that they do not want and conversion rates will suffer. With some work we could use the XAI tools and inspect a bunch of predictions to understand why the performance is dropping. But it would be far easier to resolve this issue by employing tools to detect changes in the distributions of the input data than it is to investigate the model outputs. Here, the XAI tools can only help us indirectly.
>
> From [Ambiata](https://www.ambiata.com/blog/2021-04-12-xai-part-1/)

## What about the transparency of the learning algorithm?

We can explain the algorithmic processes by which a convolutional neural network learns to classify images (e.g. initial layers that extract features).

## Are Black-Box Algorithms Even Necessary?

https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/8 

## Example from [Ambiata](https://www.ambiata.com/blog/2021-04-12-xai-part-1/)

Consider a model that predicts the top speed of a car based only on it’s colour, the number of doors it has and if the car is a convertible/cabriolet. Using XAI tools we can understand how important each variable is and the effect of each variable in the generation of each prediction to the point where we can get a good understanding of how the three variables interact. However, these tools aren’t telling us why some cars are faster than others, they are simply ‘explaining’ how the predictions were generated. We might find that the model estimates the highest top speed belongs to green convertibles with two doors. The reason why the model estimates that green two door convertibles have the highest top speed is because cars fitting that description in the dataset were mostly modern Lamborghini’s that have very powerful engines and predicting a high top speed for this cohort minimised the loss function we selected. Figure (7) provides the data generation process for this scenario, with the observed variables in the blue box. There is no causal relationship between colour and top speed, only an observed correlation. If however, we did observe ‘engine power’, we could rightly attribute a causal effect between it and top speed.

> Caution must be taken to avoid causal interpretation when it is not valid. (Green cars are not inherently faster than red ones so it is not the case that painting a car green will make it faster.)

![](https://www.ambiata.com/images/blog/xai-part-1/figure_7_thin1.png)
*Figure 7. The car manufacturer determines the colour, number of doors, whether the car is a convertible or not and the top speed. But in our dataset we only observe the variables in the blue box where there are no arrows between the predictors and top speed.*

<!-- begin admonition -->
**Relationship between explainability and impact**

Improving the explainability of a system is not a trivial task. It will require, among other things, access to technical and domain-specific expertise, resources for clear and accessible documentation, opportunities for meaningful engagement with stakeholders.

Therefore, most organisations will need to adopt a proportional approach to explainabilty (as is also the case with the remaining SAFE-D principles).

A general rule for helping assess what a proportional investment in explainability should be is the following maxim:

> The greater the impact and scope of a system, the greater the need for explainability.

Where there may be concerns about malicious attacks on the system, such as people "gaming the system", it is important to differentiate 'intepretability' of the model from related properties such as 'project transparency' or 'system accesibility'. 

For example, a model be highly intepretable but inaccessible to anyone who is not authorised to access a secure research environment. Alternatively, a model can be highly interpretable and only used internally by a team who publish limited information about their project.
<!-- end admonition -->