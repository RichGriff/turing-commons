---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 2): Project Transparency?

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situating Explanations](rri-203-4.md)

---

A team of lawyers are undertaking discovery—a process of gathering information about a case that typically requires a legal team to request information from another company or organisation (e.g. the legal team representing the other party in a dispute).
One of the areas of concern is about access to information regarding the use of an algorithmic decision-making system, which was used to make a decision that is subject to dispute in this case.
They make a request to see information about the algorithm, including how it was designed, who was involved, and how it works.

When the team of lawyers receive the requested information, there are two problems:

1. The other team have sent across mountains of documentation, in an attempt to bury any incriminating information (e.g. sending over thousands of hours of transcripts of meetings, emails, and other documents).
2. Within this documentation there is information about the structure of the algorithm, but it is written in technical jargon that is hard for the lawyers to understand.

----

This hypothetical scenario introduces two concepts of relevance to both explainability in general, and project transparency in particular:

1. *Transparency is not the same as accessibility*. In the example above, the second team of lawyers have made information accessible to the first team *in principle* but have done so in a way that few would argue is transparent in a meaningful way.[^caveat]
2. *Transparency is necessary for explainability*. The first team of lawyers will need to be able to explain the algorithm's structure in court in a way that is understandable to the judge and jury. This requires a certain degree of transparency in the documentation about how the algorithm was designed, developed, and deployed.

In the course of this section we will look at some practical choices and mechanisms by which project transparency can be achieved, and how it can improve and enhance explainability.
Before we do this, however, let's consider a question that raises questions about what we are trying to explain and where we may need transparency.

---

## What are we trying to explain? Where do we need greater transparency?

Consider the following scenario.[^ambiata]
A team of data analysts are asked to explain why predictions generated by a model used by a travel booking company are different in distribution from those observed in the past.
Perhaps the model is recommending trips to beach resorts instead of ski trips.
Here, it would be easy enough to identify that the variable `season` is an important feature used by the model that explains the change in people's purchasing behaviour, and conclude that a change in input data is responsible for the change in model's new predictions, rather than any change in the model architecture.

But now let's assume that the conversion rate (i.e. the ratio of the number of people who view a product (e.g. a holiday package) to the number who actually purchase the holiday) suddenly drops.
Again, this may seem like another case where there is a need to explain the model's behaviour in terms of an underlying shift in the data distribution.
However, this time, the problem turns out to be a fault with a third-party piece of software, used as a dependency in the team's data pipeline, which is now causing the data about a user's `location` to be incorrectly recorded.

As it turns out, the company's model has learned that those who live in affluent neighbourhoods are more likely to purchase more expensive packages—an ethically dubious practice known as personalised pricing[^pricing].
However, due to the aforementioned fault in the data pipeline, all customers are now being shown the same, more expensive, packages.
As such, fewer customers are purchasing the more reasonably priced packages and the conversion rate has dropped.

Again, there is no fault with the model's parameters here, but the change in the data's distribution this time is also not a meaningful one.
The model is still making the same predictions, but the predictions are now incorrect.

This hypothetical example exposes an important point about transparency and explainability: the locus of our explanation will not always be the model.
In many cases it can be the data pipeline used to drive the model's predictions, and a distributional shift that was not anticipated or exposed during the model's training phase.

Although we will look at model interpretability techniques in the next section, some of which can also help project teams debug or identify the source of issues, it will not always be the case that we are trying to explain the model's behaviour by explaining the model itself.

----

### What about the transparency of the learning algorithm?

Related to the previous example, it is also possible that when trying to diagnose or triage issues to explain a model's (perhaps unexpected) behaviour, the algorithm by which a model was trained can be an informative source when constructing an explanation. For instance, understanding how a convolutional neural network learns to classify images (e.g. the initial layers that extract predictive features) may help to diagnose (and subsequently explain) why a model is misclassifying images.

Here, the transparency that is required pertains to the algorithmic process by which the predictive model is trained and developed.

----

<!-- begin admonition -->
Admonition Title: Spurious Correlations

A famous example here is the instance of an algorithm that learned to classify a picture of a husky as a wolf because of the presence of snow in the background of the images. The algorithm learned that snow in the background was a good predictor of an image being of a wolf—as most wolves in the training set were show against a snowy backdrop[^wolf]. However, an over-reliance on this feature created a spurious correlation that did not hold true for the actual subjects of the images (e.g. huskies).

<!-- end admonition -->
----

As you may tell from the above two examples, the nature of our problem will determine the location of the desired transparency and the locus of our explanation.
However, this will largely remain a context-dependent issue, and so in this section we will take a simpler approach by advocating for global project transparency by default.
And, where there are overriding factors such as intellectual property concerns, or sensitive information disclosure issues, these can be evaluated by a project team on an ad hoc basis.

---

## What does responsible project transparency look like?

In a previous module, required for this module, we introduced the project lifecycle model.
This model is a useful framework for thinking about the different stages of a project, and the different types of transparency that may be required at each stage.
We will take each of the three over-arching stages in turn, and consider what transparency looks like at each stage and how it can be achieved.

### Project Design

### Model Development

### System Deployment

<!-- On trust and transparency 
Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence: https://www.nature.com/articles/s41598-023-28020-5
-->


[^caveat]: Put aside the question of whether the second team of lawyers are acting in a responsible manner here, as it would be easy to argue that they are upholding their professional duties to their client by sending over mountains of evidence.

[^ambiata]: This scenario is inspired by and adapted from [this article](https://www.ambiata.com/blog/2021-04-12-xai-part-1/): Lazaridis, D. (2021). Explainable AI (XAI) and Interpretable Machine Learning (IML) models. Ambiata. Accessed: January 22, 2023.

[^pricing]: This example refers to a practice known as 'personalised pricing', or sometimes 'price discrimination'. Neither are new practices (see [here](https://www.washingtonpost.com/archive/politics/2000/09/27/on-the-web-price-tags-blur/14daea51-3a64-488f-8e6b-c1a3654773da/)), but the widespread use of algorithmic techniques is enabling more dynamic and hyper-personalised forms of both personalised pricing and price discrimination (see [this article](https://www.washingtonpost.com/archive/politics/2000/09/27/on-the-web-price-tags-blur/14daea51-3a64-488f-8e6b-c1a3654773da/)).

[^wolf]: Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). "Why should I trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). [https://dl.acm.org/doi/abs/10.1145/2939672.2939778](https://dl.acm.org/doi/abs/10.1145/2939672.2939778)
