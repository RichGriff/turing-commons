---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 4): Situated Explanations

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situated Explanations](rri-203-4.md)

---

At the start of this module we looked at a case of generative AI producing images (or, digital art).
This is just one example of generative AI.
In addition to artwork, generative AI can also be used to create realistic images and videos of scenes or people (often referred to as 'deepfakes').
And beyond visual media, generative AI can also be used to generate [text](https://blog.google/technology/ai/bard-google-ai-search-updates/), [code](https://github.com/features/copilot), and even [music](https://soundraw.io/). Let's look at the first case.

----

At the time of writing, the most popular approach to AI generated text is the use of large language models (LLMs).
In the words of one LLM, known as [ChatGPT](https://openai.com/):

> A large language model is a machine learning model that has been trained on a large dataset of text data, and can generate or predict text. It can understand and respond to human language, and can be used for a variety of natural language processing tasks, such as language translation, text summarization, and question answering. Examples include GPT-3, BERT, T5, etc.
>
> Note: *This text was generated by an AI system (built using a large language model) in response to the following prompt, 'What is a large language model?'.*

LLMs such as ChatGPT rely on a neural network approach for training.
Specifically, they use an architecture known as a transformer.

Understanding a little about how transformers operate can help us tease out some important lessons for "situated explanations", which is the focus of this final section.

----

Transformers are trained using unsupervised learning—a type of machine learning where the learning algorithm attempts to find patterns and structure in data without explicit supervision or labelled examples. In the case of LLMs, large datasets of text are used as training data and the LLM learns to predict the next word in a sentence (i.e. the prompt) based on the preceding words. The key innovation of the transformer architecture is the use of so-called "attention mechanisms".

These mechanisms are an important part of how LLMs can be made interpretable, although it is important to remember that LLMs are first and foremost an example of *black box systems*—a point we will return to shortly.
The attention mechanism used by transformers allows the model to "attend" to specific parts of the input (e.g. the sequence of word, or 'tokens') when making predictions.
Different weights are then applied to each position in the sequence based on its relevance to the current context.

----

<!-- begin admonition -->
**What are tokens?**
In natural language processing, an input sequence is typically divided into individual units of meaning called "tokens".
Depending on the task, these tokens can be words, subwords, or individual characters.
LLMs such as GPT-3 and BERT are two examples of NLP systems.
<!-- end admonition -->

----

You will notice here, a similar process used by some of the other algorithmic approaches and models discussed in the previous sections.
However, the attention mechanism that neural-network based LLMs use is applied to multiple layers of the transformer's architecture, which allows the model to capture *long-range dependencies* and encode complex relationships between different parts of the input sequence.
This process can be repeated millions (or sometimes billions) of times on a vast datasets of text until the model has learned to generate coherent and meaningful sentences.

This modern approach to NLP is one of the primary mechanisms that allows LLMs to generate such impressive results.
However, the complexity of the transformer architecture, combined with the attention mechanism and the vast training datasets, means that it is very difficult to understand exactly how the model is making its predictions.

---

## Explainable Processes or Explainable Outcomes

By now, you may have noticed that there are two over-arching targets for explainability:

- The processes by which a model and it's encompassing system/interface are designed, developed, and deployed.
- The outcomes of the predictive model, which may be responsible for automated decisions or feed into human decision-making (e.g. decision support systems, visualisations).

This distinction between process-based explanations and outcome-based explanations was put forward in guidance, titled 'Explaining decisions made with AI', which was developed by the Information Commissioner's Office (ICO) and the Alan Turing Institute.

----

The distinction helps to make clear some of the limitations referred to over the course of this module (e.g. limitations of post hoc methods of model interpretability).
For example, a local, post hoc method will support explanations about why a model produced a specific prediction.
This is in one sense, an *outcome-based* explanation, but it may also be incomplete.
Consider a predictive model that classifies surveillance images as depicting illegal deforestation or not.
The outcome may be a binary classification (i.e. yes/no), perhaps accompanied by a confidence score.
But, the tangible real world consequences of this classification may be very different depending on the context in which the model is used.
For instance, due to regulatory and compliance differences, two national bodies may choose to do very different things with this prediction.
One may choose to notify local authorities to investigate, whereas the other may choose to do nothing.

This simple (hypothetical) example shows that there is a further *sociotechnical outcome* that encompasses but goes beyond the outcome of the model itself.
And, furthermore, depending on who the recipient of this outcome is, there may also need to be a complementary explanation about the processes of project design, model development, and system deployment.

----

Consider again the above example of two national bodies receiving predictions about illegal deforestation.
Let's look at why the second body chose to do nothing.
Maybe they have a policy of not taking action on predictions with a confidence score below some threshold without additional assurance of how the confidence score was calculated.
As such, they may have a need for an explanation about the following processes:

- Which experts were involved in the design of the system, and how did their involvement lead to the choice of decision threshold for the classifier?
- Which evaluation metrics were selected to assess the performance of the model, and why were these metrics chosen? How were biases such as training-serving skew assessed and mitigated?
- Were any steps taken during the model's implementation to accommodate variations in the quality of input data (e.g. low resolution images)?

----

All of these questions are requests for an explanation.
Furthermore, they are all requests for an explanation about the processes that may or may not have contributed to the outcomes of the model.

Because the recipients of the explanations have needs that may be different to those with technical expertise and deep familiarity with the model's architecture, it is not sufficient to simply provide an outcome-based explanation that relies on the outputs of post hoc methods of model interpretability.
Had the request for an explanation come from a machine learning engineer, the questions may have been very different.

----

This is why we refer to 'situated explanations'.
The term 'situated' is used to describe the context in which explanations are requested, and the stakeholder that is being engaged.
Having an awareness and appreciation of these contextual factors is important for identifying what the 'explainability' requirements for a project will be, and can only be suitably addressed through informed and meaningful stakeholder engagement—ideally, early on in the project's lifecycle and in an ongoing and iterative manner.

---

## The Importance of Trust

- Are Black-Box Algorithms Even Necessary? https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/8 
  - Consider the example of the surgeon and robot presented in the above article? Should we trust the 2% inaccuracy rate?
  - What subpopulation do we come from?
    - Here's a toy example:
    - ![](../../../assets/images/../../../docs/assets/images/graphics/class-imbalance.png)
    - If you are a member of the majority class, you might be happy with a 2% error rate.
    - But if you belonged to the minority class, you would care a lot more about the clearly awful coarse-grained model.

<!-- On trust and transparency 
Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence: https://www.nature.com/articles/s41598-023-28020-5
-->



<!-- begin admonition -->
**Relationship between explainability and impact**

Improving the explainability of a system is not a trivial task. It will require, among other things, access to technical and domain-specific expertise, resources for clear and accessible documentation, opportunities for meaningful engagement with stakeholders.

Therefore, most organisations will need to adopt a proportional approach to explainability (as is also the case with the remaining SAFE-D principles).

A general rule for helping assess what a proportional investment in explainability should be is the following maxim:

> The greater the impact and scope of a system, the greater the need for explainability.

Where there may be concerns about malicious attacks on the system, such as people "gaming the system", it is important to differentiate 'interpretability' of the model from related properties such as 'project transparency' or 'system accessibility'.

For example, a model be highly intepretable but inaccessible to anyone who is not authorised to access a secure research environment. Alternatively, a model can be highly interpretable and only used internally by a team who publish limited information about their project.
<!-- end admonition -->
