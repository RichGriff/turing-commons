---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 4): Situated Explanations

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situated Explanations](rri-203-4.md)

---

At the start of this module we looked at a case of a generative AI producing images (or, digital art) that were difficult to explain.

Image creation is just one example of generative AI's capabilities.
In addition to artwork, generative AI can also be used to create realistic images and videos of scenes or people (often referred to as 'deepfakes').
And beyond visual media, generative AI can also be used to generate [text](https://blog.google/technology/ai/bard-google-ai-search-updates/), [code](https://github.com/features/copilot), and even [music](https://soundraw.io/). Let's look at the first case, text.
Most, if not all, forms of generative AI suffer the same explainability barrier as the image generation example.
However, this does not mean that the requirements for explainability are the same.
This is because the sociocultural context in which the systems are deployed create different expectations for valid and accessible explanations.
Therefore, we need to consider the *situation* in which the system is deployed, and the *users* of the system, when we think about explainability.
In other words, we need to understand what is required for the delivery and communication of *situated explanations*.

----

<!-- begin admonition -->
**Ethics of Generative AI**

There is a lot to say about the ethics of generative AI, outside of the context of explainability.
However, as this would be tangential to our current focus for this section, we will not say more about this beyond the need to maintain a critical perspective on its development and use.
The following resources are useful for further reading:

<!-- add further resources -->

<!-- end admonition -->
----

## Building upon interpretability and transparency to develop situated explanations

At the time of writing, the most popular approach to AI generated text is the use of large language models (LLMs).
In the words of one LLM, known as [ChatGPT](https://openai.com/):

> A large language model is a machine learning model that has been trained on a large dataset of text data, and can generate or predict text. It can understand and respond to human language, and can be used for a variety of natural language processing tasks, such as language translation, text summarization, and question answering. Examples include GPT-3, BERT, T5, etc.
>
> Note: *This text was generated by an AI system (built using a large language model) in response to the following prompt, 'What is a large language model?'. You can notice that there are dubious statements and possible inaccuracies, such as the claim about LLMs being able to "understand" human language. This is why it is always important to ensure that LLMs and other forms of generative AI are used responsibly, and that their limitations are well understood.*

LLMs such as ChatGPT rely on a neural network approach for training the model.
Specifically, they use an architecture known as a *transformer*.

Understanding a little about how transformers operate can help us tease out some important lessons for "situated explanations".
Therefore, we're going to explore a short tangent before coming back to the main topic of this section.

----

### Transformers and LLMs

Transformers are trained using unsupervised learning—a type of machine learning where the learning algorithm attempts to find patterns and structure in data without explicit supervision or labelled examples.
In the case of LLMs, large datasets of text are used as training data and the LLM learns to predict the next word in a sentence (i.e. the prompt) based on the preceding words.
The key innovation of the transformer architecture is the use of so-called "attention mechanisms".

These mechanisms are an important part of how LLMs can be made interpretable, although it is important to remember that LLMs are first and foremost an example of *black box systems*—a point we will return to shortly.
The attention mechanism used by transformers allows the model to "attend" to specific parts of the input (e.g. the sequence of word, or 'tokens') when making predictions.
Different weights are then applied to each position in the sequence based on its relevance to the current context.
This process can be repeated millions (or sometimes billions) of times on a vast datasets of text until the model has learned to generate coherent and meaningful sentences.

<!-- insert explanatory graphic -->

----

<!-- begin admonition -->
**What are tokens?**
In natural language processing, an input sequence is typically divided into individual units of meaning called "tokens".
Depending on the task, these tokens can be words, subwords, or individual characters.
LLMs such as GPT-3 and BERT are two examples of NLP systems.
<!-- end admonition -->

----

You will notice here, a similar process used by some of the other algorithmic approaches and models discussed in the previous sections: feature summary statistics and values for model internals are popular approaches to interpretability.
However, the attention mechanism that neural-network based LLMs use is applied to *multiple layers* of the transformer's architecture.
This allows the model to capture *long-range dependencies* and encode complex relationships between different parts of the input sequence, which in turn allows LLMs to produce more coherent and meaningful text.
For instance, there is a substantive difference in the following two tasks, both of which involve a prompt:

1. Complete the sentence: "Mark is looking for ... keys."
2. Write me a press release for a new quarterly earnings report. The company is called 'Widgets Inc.' The profits are up 20% from last quarter.

In the first example, filling the gap with the word "his" is sufficient to complete the sentence and the important features would probably be 'Mark' and the target object, 'keys'.
Such an approach would also allow members of a project team to easily identify gender bias in the model (e.g. associating specific genders with social roles, such as doctor or nurse).

However, in the second example, the task is much more complex, and the space of valid responses to the prompt is significantly more diverse.
As such, simply showing the model's attention weights—calculated by comparing each token to all other tokens in the sequence and measuring the similarity between them—leads to a radically underdetermined explanation for why a particular answer was given.

----

While we have used LLMs as our illustrative example here, the same issues occur with many other types of black box AI systems, and especially with neural networks.
This raises an important question:

> If we can't fully understand or interpret the behaviour of a model or AI system, what supplementary information can we provide to ensure the behaviour and outcomes of the system are explainable?

---

### Explainable Processes or Explainable Outcomes

By now, you may have noticed that there are two over-arching targets for explainability:

- The processes by which a model and it's encompassing system/interface are designed, developed, and deployed.
- The outcomes of the predictive model, which may be responsible for automated decisions or feed into human decision-making (e.g. decision support systems, visualisations).

This distinction between process-based explanations and outcome-based explanations was put forward in guidance, titled 'Explaining decisions made with AI', which was developed by the Information Commissioner's Office (ICO) and the Alan Turing Institute.
It is also how we will understand the differential needs for situated explanations.

----

The distinction helps to make clear some of the limitations referred to over the course of this module (e.g. limitations of post hoc methods of model interpretability).
For example, a local, post hoc method will support explanations about *why a model produced a specific prediction*.
This is in one sense, an *outcome-based* explanation, but it is also an incomplete outcome-based explanation.

To see why, consider a predictive model that classifies surveillance images as depicting illegal deforestation or not.
The outcome may be a binary classification (i.e. yes/no), perhaps accompanied by a confidence score.
But, the tangible real world consequences of this classification may be very different depending on the context in which the model is used.
For instance, due to regulatory and compliance differences, two national bodies may choose to do very different things with this prediction.
One may choose to notify local authorities to investigate, whereas the other may choose to do nothing.

----

This simple (hypothetical) example shows that there is a further *societal outcome* that encompasses, but goes beyond, the outcome of the model itself.
And, in between the model outcome and the societal outcome, there may also be a salient system outcome, depending on how the model has been implemented within the system that user's interact with (e.g. a recommendation system or a decision support system that serves as the interface between the model and the user).

![](https://raw.githubusercontent.com/alan-turing-institute/turing-commons/main/docs/assets/images/graphics/model-outcome.png)

----

Explaining the different outcomes (model, system, societal) will likely require varying levels of detail and granularity and may require those doing the explaining to refer to different processes across the project lifecycle. 

For instance, asking the question "why did the model classify an image as an instance of illegal deforestation?" may require a more localised explanation about the model's internal processes and the data it was trained on.
But asking the question, "why did the national body choose to do nothing on the basis of a prediction of illegal deforestation?" will require a more extensive and wide ranging explanation about how the system was designed, developed, and deployed, as well as an understanding of the societal and regulatory context in which the system is used.
While the original request for an explanation appears, at first glance, to be a request for an outcome-based explanation, further investigation could reveal that the request is really about the processes that led to the outcome (e.g. how confidence scores were selected, whether the training data was sufficiently representative, or whether the system is used by skilled professionals who can spot false positives).

----

<!-- begin admonition -->
**Using the project lifecycle model to support explanations**

The project lifecycle model introduced in the previous model provides a scaffold for thinking about which processes may be the target of an explanation.
For example, each stage can serve as a deliberative prompt to help identify whether the typical tasks carried out during that stage could provide salient information to help address the request for an explanation.

<!-- end admonition -->

----

Consider again the above example of two national bodies receiving predictions about illegal deforestation.
Let's look at why the second body chose to do nothing.
Maybe they have a policy of not taking action on predictions with a confidence score below some threshold without additional assurance of how the confidence score was calculated.
As such, they may have a need for an explanation about the following processes:

- Which experts were involved in the design of the system, and how did their involvement lead to the choice of decision threshold for the classifier?
- Which evaluation metrics were selected to assess the performance of the model, and why were these metrics chosen? How were biases such as training-serving skew assessed and mitigated?
- Were any steps taken during the model's implementation to accommodate variations in the quality of input data (e.g. low resolution images)?

----

All of these questions are requests for an explanation.
Furthermore, they are all requests for an explanation about the processes that may or may not have contributed to the outcomes of the model.

Because the recipients of the explanations have needs that may be different to those with technical expertise and deep familiarity with the model's architecture, it is not sufficient to simply provide an outcome-based explanation that relies on the outputs of post hoc methods of model interpretability.
Had the request for an explanation come from a machine learning engineer, the questions may have been very different.

----

This is why we refer to 'situated explanations'.
The term 'situated' is used to describe the context in which explanations are requested, and the stakeholder that is being engaged.
Having an awareness and appreciation of these contextual factors is important for identifying what the 'explainability' requirements for a project will be, and can only be suitably addressed through informed and meaningful stakeholder engagement—ideally, early on in the project's lifecycle and in an ongoing and iterative manner.

---

## The Importance of Trust

- Are Black-Box Algorithms Even Necessary? https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/8 
  - Consider the example of the surgeon and robot presented in the above article? Should we trust the 2% inaccuracy rate?
  - What subpopulation do we come from?
    - Here's a toy example:
    - ![](../../../assets/images/../../../docs/assets/images/graphics/class-imbalance.png)
    - If you are a member of the majority class, you might be happy with a 2% error rate.
    - But if you belonged to the minority class, you would care a lot more about the clearly awful coarse-grained model.

<!-- On trust and transparency 
Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence: https://www.nature.com/articles/s41598-023-28020-5
-->



<!-- begin admonition -->
**Relationship between explainability and impact**

Improving the explainability of a system is not a trivial task. It will require, among other things, access to technical and domain-specific expertise, resources for clear and accessible documentation, opportunities for meaningful engagement with stakeholders.

Therefore, most organisations will need to adopt a proportional approach to explainability (as is also the case with the remaining SAFE-D principles).

A general rule for helping assess what a proportional investment in explainability should be is the following maxim:

> The greater the impact and scope of a system, the greater the need for explainability.

Where there may be concerns about malicious attacks on the system, such as people "gaming the system", it is important to differentiate 'interpretability' of the model from related properties such as 'project transparency' or 'system accessibility'.

For example, a model be highly intepretable but inaccessible to anyone who is not authorised to access a secure research environment. Alternatively, a model can be highly interpretable and only used internally by a team who publish limited information about their project.
<!-- end admonition -->
