---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 3): Model Interpretability

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situating Explanations](rri-203-4.md)

---

Here's a hypothetical scenario adapted from the same example provided in [this article]((https://www.ambiata.com/blog/2021-04-12-xai-part-1/)), which serves as a cautionary tale for the remainder of this section.

A model predicts the top speed of a car based on the following three features:

1. `Colour`
2. `Number of doors`
3. `Convertible (yes/no)`

A team of data analysts want to employ interpretability methods to understand how important each of these features are to the prediction and their relative effects.
In doing so, they find that the model estimates the highest top speed to cars that are green, have two doors, and are convertibles.
However, although the use of the model interpretability method here "explaining" how the model's predictions are generated, it is not explaining why some cars are faster than others.

----

As the original article states:

> The reason why the model estimates that green two door convertibles have the highest top speed is because cars fitting that description in the dataset were mostly modern Lamborghiniâ€™s that have very powerful engines and predicting a high top speed for this cohort minimised the loss function we selected.
> (Lazaridis, 2021)

The following diagram from the same article illustrates the problem further:

![](https://www.ambiata.com/images/blog/xai-part-1/figure_7_thin1.png)
*Figure 7. The car manufacturer determines the colour, number of doors, whether the car is a convertible or not and the top speed. But in our dataset we only observe the variables in the blue box where there are no arrows between the predictors and top speed.*

----

We won't belabour the well-trodden point about correlation not equalling causation in this section.
However, we do wish to draw attention to the limitation of model interpretability methods to causally explain the relationship between the input features and the model's predictions, as a gentle reminder about the importance of understanding the limitations of any tool before using it.

<!-- begin admonition -->
Title: Causal Inference in Machine Learning and AI

The topic of causal inference in machine learning and AI is a well-studied area of research, but is not within the scope of this section or module.

For more information, see the following resources:

- Pearl and Mackenzie (2018) The Book of Why: The New Science of Cause and Effect.
- Sgaier, Huang and Charles (2020) The Case for Causal AI. Stanford Social Innovation Review. [https://ssir.org/articles/entry/the_case_for_causal_ai](https://ssir.org/articles/entry/the_case_for_causal_ai)
- Forney and Mueller (2022) Causal inference in AI education: A primer. Journal of Causal Inference. [https://www.degruyter.com/document/doi/10.1515/jci-2021-0048/html](https://www.degruyter.com/document/doi/10.1515/jci-2021-0048/html)

<!-- end admonition -->

---

Software packages for model interpretability: 

- https://github.com/interpretml/interpret#supported-techniques 
- documentation: https://interpret.ml/docs/intro.html 

Model interpretabilty is important for practical reasons such as debugging (e.g why is a model performing poorly? How can it be improved?). However, here we will focus on the ethical issues, or those that fall under the banner of responsible research and innovation.

<!-- start admonition -->
**Realtions Between Weights and Explanations**
Consider a simple linear regression model such as the following, which predicts the expected time in which a runner will finish a marathon:

$$ \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + [...] + \beta_n x_n $$

Let's assume that the input variable $$x_1$$ refers to the number of training sessions the runner has completed in the past year. And, let's assume that the model has learned that the more training sessions a runner has completed, the faster they will finish the marathon, as evidence by a positive weight for $$\beta_1$$.

So far, so good.

[...]

What appears to be an unimportant feature at first turns out not to be the case. It is only when we learn about the relationship between the two variables that we understand the relative importance of the weights. As Christopher Molnar states:

> The interpretation of a single weight always comes with the footnote that the other input features remain at the same value, which is not the case with many real applications [...] The weights only make sense in the context of the other features in the model.[^molnar2022]

## On Responsible Model Selection


[^molnar2022]: Molnar, C. (2022). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html
