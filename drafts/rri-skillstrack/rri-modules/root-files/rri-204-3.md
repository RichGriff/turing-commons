---
slideOptions:
  transition: slide
  spotlight:
    enabled: true
---

# Explainability (Section 3): Model Interpretability

> **Note**
> The following sections are part of this module:
>
> - Section 1: [What is Explainability?](rri-203-1.md)
> - Section 2: [Project Transparency](rri-203-2.md)
> - Section 3: [Model Interpretability](rri-203-3.md)
> - Section 4: [Situating Explanations](rri-203-4.md)

---

Here's a hypothetical scenario, summarised from the same case presented in [this article](https://www.ambiata.com/blog/2021-04-12-xai-part-1/).
It serves as a cautionary tale for the remainder of this section.

A model, trained on a dataset of cars, predicts a car's `top speed` based on the following three features:

1. `Colour`
2. `Number of doors`
3. `Convertible` (binary value: 'yes' or 'no')

Surprisingly, the model does very well for cars that are green, have two doors, and are convertibles—it predicts they have very high speeds.

A team of data analysts are surprised by this and want to employ model interpretability methods to understand how important each of these features are to the prediction and their relative effects.
It turns out that all of the features are important, but none of them are "explaining" why such cars are faster than others.

----

As you may have guessed, the real reason that the model is accurate in predicting a high top speed in such cases is because most of the cars that are 'green', 'have 2 doors', and are 'convertibles' happen to be Lamborghinis, which are known for their high top speeds.
However, none of the features in the model have any causal relationship with the top speed of a car.
Rather, it is the engine (among other things) that is responsible for the top speed of a car.
The other features are what we would call 'confounding variables'—they are not the cause of the top speed, but they are correlated with the cause.

----

Here is a diagram (reprinted from the [same article](https://www.ambiata.com/blog/2021-04-12-xai-part-1/) above) that illustrates the problem:

![](https://www.ambiata.com/images/blog/xai-part-1/figure_7_thin1.png)
*Figure 7. The car manufacturer determines the colour, number of doors, whether the car is a convertible or not and the top speed. But in our dataset we only observe the variables in the blue box where there are no arrows between the predictors and top speed.*

----

<!-- begin admonition -->

**What is a feature? What is a variable?**

For those who are new to machine learning, the difference between 'features' and 'variables' can appear confusing, especially because the two are sometimes used synonymously.
However, the concepts can come apart, and the term 'feature' has a specific meaning that goes beyond the general meaning of 'variable' as employed in statistics more generally.

In short, a feature is some measurable property or characteristic of the data that are used as input for a machine learning algorithm to make predictions or decisions.
In a simple example, the set of features could be the columns in a dataset that represent properties of an object (e.g. `number of doors`, `colour`, and `convertible` in the previous example).
However, features are not always hand selected by the ML engineers.
For more complex algorithms, features can be selected or engineered through computational methods, resulting in features that are not interpretable by humans.

As such, in some instances the variables contained in the original dataset, may end being the same as the variables (or, features) that are used as inputs into the model.
However, in other cases, many of the original set of variables that may be explored during initial data analysis can be discarded based on their lack of relevance or contribution to model performance.
Therefore, in machine learning, the term 'feature' is used *specifically* to refer to those input variables that are used for making predictions.

<!-- end admonition -->

----

We won't belabour the well-trodden point about correlation not equalling causation in this section.
However, it is a useful means for drawing attention to the limitation of model interpretability methods to explain causal relationships between the input features and the model's predictions, among other types of explanations (e.g. explanations about data collection issues, or project governance choices).
Moreover, it serves as a gentle reminder about the importance of understanding the limitations of any tool before using it.

----

In this section, we will explore the more technical aspects of explainability, which rely on the interpretability of the underlying model (including its development and architecture).
This section does not go into the practical details of how to use interpretability methods, but does provide a brief overview of some of the most common methods and tools.
As always, the focus is on the ethical implications of these methods and how they help promote a responsible approach to data science and AI research and innovation.

----

<!-- begin admonition -->
Title: Causal Inference in Machine Learning and AI

The topic of causal inference in machine learning and AI is a well-studied area of research, but is not within the scope of this section or module.

For more information, see the following resources:

- Pearl and Mackenzie (2018) The Book of Why: The New Science of Cause and Effect.
- Sgaier, Huang and Charles (2020) The Case for Causal AI. Stanford Social Innovation Review. [https://ssir.org/articles/entry/the_case_for_causal_ai](https://ssir.org/articles/entry/the_case_for_causal_ai)
- Forney and Mueller (2022) Causal inference in AI education: A primer. Journal of Causal Inference. [https://www.degruyter.com/document/doi/10.1515/jci-2021-0048/html](https://www.degruyter.com/document/doi/10.1515/jci-2021-0048/html)

<!-- end admonition -->

---

## What is model interpretability?

In the [opening section](rri-204-1.md) of this module, we introduced the following definition of interpretability to help distinguish it from explainability:

> Interpretability is the degree to which a human can understand the cause of a decision.
> -- Miller (2017)

In this section, when we use the term 'interpretability' we will be referring to **model** interpretability, which is the degree to which a human can understand the cause of a model's prediction or behaviour.
There are, however, several types of interpretability that need to be distinguished from one another.

----

### Understanding types and methods of interpretability

Christopher Molnar's [excellent book on Interpretable ML](https://christophm.github.io/interpretable-ml-book/) provides several useful ways to differentiate model interpretability, which we will take as our starting point:

- Intrinsic vs Post Hoc
- Model-specific vs model-agnostic
- Global vs Local
- Results of interpretability methods

Before we turn to these elements it is important to note that while grouped together into a taxonomy, the different elements are not necessarily of the same type.
For instance, 'model-specific vs model agnostic' refers to a means for understanding the method by which interpretability is achieved, whereas 'results of interpretability methods' refers to the type of output that is produced by the interpretability method.

The taxonomy is nevertheless highly informative, and so we will now look at each of the elements in further detail.

----

#### Intrinsic versus post hoc

Intrinsic interpretability is a property of a model that can be measured on a scale from the class of models that are *instrinically* interpretable (e.g. simple logistic regression models used for binary classification), through to the class of models that are highly complex and have very low (to no) intrinsic interpretability (e.g. large language models).
For example, the following decision tree would have high levels of intrinsic interpretability because of its simple structure.

![A graphic showing a decision tree where the branches split based on whether a feature exceeds some threshold or not]()

In contrast, post hoc interpretability is a property of the model that is subject to the application of additional methods or techniques applied after a model's training (hence, post hoc).
For example, random forests—a type of machine learning method that is known as an 'ensemble method' because it combines the predictions of several simpler models (i.e. decision trees) to predict an outcome (based on the consensus among the different trees within the forest)—have low levels of intrinsic interpretability.
However, post hoc methods (e.g. [LIME](https://github.com/marcotcr/lime)) can be applied to help extract interpretations about, say, the feature importance, which would otherwise be too complex to extract.

----

<!-- begin admonition -->

Clarifications

> Two clarifications are worth making:
> 1. The two types of intepretability are not mutually exclusive. Post hoc methods can also be applied to intrinsically interpretable models to improve interpretability.
> 2. Intrinsically intepretable models may also depend on the use of techniques to improve interpretability, but such methods would have been applied prior to training (e.g. feature engineering) for the model to remain *intrinsically* interpretable.

<!-- end admonition -->

----

#### Model-specific versus model-agnostic

There are a variety of ML models, ranging from simple logistic regression through gradient boosting and to more complex forms of reinforcement learning involving linked neural networks.
The application of interpretability techniques and methods is sometimes constrained by the choice of model.
For instance, integrated gradients for neural networks is a technique for visualising feature importance, as is the case with the following image, where the technique allows the interpreter to see which pixels were important in an image classification task.

![Example of integrated gradients technique showing which parts of an image were important in determining the output of a classification task.](https://www.tensorflow.org/static/tutorials/interpretability/images/IG_fireboat.png)

This is an example of a *model-specific* method, because it requires a calculation to be carried out on specific elements of the underlying neural network, and this method would not apply to other models that have different internal structures.
This is the case even if the objective of *determining feature importance* is similar to the goal of other methods (e.g. LIME).

In contrast, model agnostic methods can be used for any model.
As such, they do not utilise any structural information about the model, but typically treat the algorithm as a black box and just focus on understanding which data, inputs, or features were important in the model's prediction.

A common model agnostic method is to compute a partial dependence plot (PDP) to help visualize the relationship between a feature and a model's prediction.
It shows how the model's prediction changes as the value of a feature changes, holding all other features constant.

----

<!-- begin admonition -->

**Cautionary Tale Redux**

Recall the cautionary tale at the start of this section!
Model agnostic methods, such as PDPs, may not help uncover any causal relationships between features and outcomes, as the following example (reprinted from [Molnar (2019)](https://christophm.github.io/interpretable-ml-book/pdp.html)) shows:

![A partial dependence plot showing the cancer probability and the interaction of age and number of pregnancies](https://christophm.github.io/interpretable-ml-book/images/pdp-cervical-2d-1.jpeg)
*PDP of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had 0 or more than 2 pregnancies. But be careful when drawing conclusions: This might just be a correlation and not causal!*

<!-- end admonition -->

----

#### Global Versus Local

Finally, there is the distinction between local and global scope of interpretability methods.

The example of the partial dependence plot given in the previous section was an example of a global method. This is because it helps individuals to interpret the relationship between a feature and a model's prediction, independent of any specific observation. In other words, it focuses on the global interpretability of the model itself.

In contrast, there are also local methods that help interpret why *specific* predictions were made for a particular observation. Rather than focusing on the model itself, here the focus is on an individual data point or observation.

%%ethical significance of the distinctions%%

Both types of interpretability are likely to be useful, but for different circumstances.  For instance, let's assume a user receives a classification that they wish to contest. Here, local interpretability methods can help the system owner understand why a particular instance was classified as belonging to one bucket, rather than another. This type of interpretation is likely to be more relevant in forming an explanation that is deemed adequate by the affected user than an explanation that is supported by a global interpretation.

However, if an auditor or procurer is looking to assess the overall robustness and validity of a model as part of an evaluation process, they are more likely to want to understand, say, the overall relationships between features and predictions, which can be generated by global interpretability methods.

----

### Outcomes of Interpretability Methods

Finally, Molnar introduces five different outcomes that can be realised through the application of interpretability methods. These are:

1. Feature summary statistics: a variety of metrics that can be used to help quantity the importance of features (e.g. a score that shows the increase in prediction error following permutation to a feature's value), or a pairwise interaction strength between two features.
2. Feature summary visualisations: a different representational format that can also be used to help illustrate many of the feature summary statistics, in a way that can be more meaningful to a human.
3. Model internals: values or information used to represent some element in the model's structure, such as its parameter (e.g. weights and biases). For intrinsically interpretable models, such as linear regression models, this is how the model is said to be interpretable.
4. Data points: techniques that return data points as output fall under this category. For instance, counterfactual explanations return a data point that is similar to the data point being queried, but where the variation results in a different prediction.
5. Intrinsically interpretable models: an interpretable model itself constitutes a result of some method, even if the method was to explicitly choose to develop a model that required no additional *post hoc* methods to be applied.

---

Software packages for model interpretability: 

- https://github.com/interpretml/interpret#supported-techniques 
- documentation: https://interpret.ml/docs/intro.html 

Model interpretabilty is important for practical reasons such as debugging (e.g why is a model performing poorly? How can it be improved?). However, here we will focus on the ethical issues, or those that fall under the banner of responsible research and innovation.

<!-- start admonition -->
**Relation Between Weights and Explanations**
Consider a simple linear regression model such as the following, which predicts the expected time in which a runner will finish a marathon:

$$ \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + [...] + \beta_n x_n $$

Let's assume that the input variable $$x_1$$ refers to the number of training sessions the runner has completed in the past year. And, let's assume that the model has learned that the more training sessions a runner has completed, the faster they will finish the marathon, as evidence by a positive weight for $$\beta_1$$.

So far, so good.

[...]

What appears to be an unimportant feature at first turns out not to be the case. It is only when we learn about the relationship between the two variables that we understand the relative importance of the weights. As Christopher Molnar states:

> The interpretation of a single weight always comes with the footnote that the other input features remain at the same value, which is not the case with many real applications [...] The weights only make sense in the context of the other features in the model.[^molnar2022]

## Model interpretability as a contributor to responsible research and innovation

In this section, so far, we have looked at understanding what model interpretability is and some of the strengths and limitations of interpretability methods.
Now, let's us apply some of this knowledge to our main objective: understanding how model interpretability supports explainability and contributes to a responsible process of research and innovation.

In short, model interpretability and the associated methods are necessary to ensure the explainable design, development, and deployment of data-driven technologies.
Here again, we see a major difference between our use of 'interpretability' and 'explainability'.
Whereas, we reserve the concept of interpretability for the methods and techniques that are used to understand the inner workings of a model, explainability has a broader scope and refers to any process or mechanism across the whole project lifecycle that supports communication and reason-giving between project team members and stakeholders.

---

[^molnar2022]: Molnar, C. (2022). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html
